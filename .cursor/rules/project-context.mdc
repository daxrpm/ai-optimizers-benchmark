---
alwaysApply: true
---

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024
\usepackage{stfloats}
\usepackage{cite}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{afterpage}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Toward Computationally Efficient AI: A Comparative Study of Optimization Paradigms in DNN Training}\\

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

%\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
%\and
%\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}

\maketitle

\begin{abstract}
%However, access to high-performance computational infrastructure is usually unequal across academic institutions and organizations.
The rapid expansion of artificial intelligence (AI) has significantly increased the computational demands associated with training advanced models. Moreover, the use of inefficient training techniques not only raises operational costs but also contributes to unnecessary energy consumption and environmental impact. 
Therefore, identifying scalable and resource-efficient optimization strategies is critical to enhancing the accessibility, affordability, and sustainability of AI-driven digital services. 
Despite the proliferation of optimization paradigms for AI, existing evidence remains fragmented, offering limited practical guidance on their trade-offs in terms of accuracy, convergence time, and computational cost. 
To address this gap, this study presents a comprehensive comparative evaluation of optimization paradigms applied to different types of Deep Neural Networks (\textit{DNNs}), which are widely used to process data with a grid-like topology, such as images and videos. 
We evaluate representative first-order and second-order methods, including Adam, Kronecker-Factored Approximate Curvature (K-FAC), and the recently proposed AdaMuon optimizer. 
Our analysis combines mathematical characterization with empirical evaluation across multiple datasets, measuring predictive accuracy, convergence speed, computational overhead, and memory consumption. 
The results indicate that, while K-FAC achieves competitive accuracy, it incurs substantial computational and memory overhead. 
AdaMuon, on the contrary, maintains a near first-order computational cost while delivering an improved efficiency–accuracy balance. 
In single-GPU settings, AdaMuon demonstrates a more favorable trade-off between performance and resource consumption, whereas K-FAC remains constrained by scalability limitations. 
In this context, Adam provides a robust and scalable baseline in single-GPU environments, exhibits strong empirical convergence speed and stable optimization dynamics due to its adaptive per-parameter learning rates and moment-based updates, achieving competitive accuracy with moderate computational overhead. 
These findings provide initial guidance for selecting optimization strategies that balance accuracy, convergence speed, and computational efficiency in \textit{(DNN)} applications, supporting more sustainable and cost-effective AI deployment.


\end{abstract}

\begin{IEEEkeywords}
Computational Efficiency, Deep Neural Networks, Optimization Paradigms,  Sustainability
\end{IEEEkeywords}

\section {Introduction}

During the last few years, AI has rapidly expanded across the globe and has become deeply integrated into multiple industries, academic fields, and daily life. 
In healthcare, its presence is significant. 
In the US, the number of AI-enabled medical devices approved by the FDA rose from just six in 2015 to 233 in 2023, reflecting a significant acceleration in their acceptance. 
Similarly, in Germany, AI has achieved an adoption rate of over 50\% within the medical field \cite{maslej2025, patel2025}.

Beyond healthcare, AI has also influenced the transportation sector. 
The self-driving car business is no longer limited to experimentation. 
For instance, Waymo, a major autonomous taxi service provider, now completes approximately 150,000 rides per week. 
%GS: Agregar una referencia en la oración anterior.
In scientific research, AI continues to expand the limits of innovation generating successful protein binders through AlphaProteo, and even contributing to biomedical research by becoming AI-powered scientists themselves \cite{maslej2025}.

The impact of AI is also evident in agriculture. Thanks to its advanced monitoring and precision farming capabilities, it has become a critical tool for improving efficiency and productivity. This is reflected in the high adoption rates in countries such as China and India, where AI usage in agriculture reaches 51.40\% and 50.93\%, respectively \cite{patel2025}. 

Together, these developments show that AI has moved from being an emerging technology to becoming a central part of today's economy and scientific research. 
In this context of widespread use, Convolutional Neural Networks (\textit{CNNs}) have become essential tools for image classification in a wide range of domains, including engineering, security, medical testing, military applications, and deepfake detection \cite{alrajeh2025,chen2021,elngar2021}. 
%GS: revisar la primera referencia alrajeh2025
Additionally, although to a lesser extent, CNNs have also been applied in other areas, offering valuable insights in fields such as natural language processing, audio analysis, and time-series forecasting (\textit{e.g.}, market prediction) \cite{ersavas2024}. 

%AI models are becoming increasingly demanding and energy-intensive. For instance, -->LR --> hemodificado un poco este párrafo
AI models are becoming increasingly resource-intensive and therefore energy-intensive. 
Indeed, the training computation required for large-scale AI models roughly doubles every five months, resulting in a significant increase in annual power consumption\cite{maslej2025}. 
Thus, despite their versatility and effectiveness, Deep Neural Networks (\textit{DNNs}) pose environmental sustainability concerns due to their inherently high computational cost \cite{alrajeh2025}.

%In this context, improving training efficiency becomes not only a performance objective but also a sustainability goal.  -->LR
In this context, improving training efficiency is not only a performance goal but also a sustainability goal. Optimization plays an essential role for an effective DNN training.
However, first-order methods, such as Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam), have key limitations, which motivates the exploration of second-order optimizers for improved convergence and generalization~\cite{Dubey2019,Reyad2023,Habib2020,Gasse2019,Cong2022}.

Optimization determines how efficiently and accurately \textit{(DNNs)} learn from data. 
The high dimensionality and non-convexity of \textit{(DNNs)} loss landscapes make optimization a central challenge, directly impacting convergence speed, final accuracy, and generalization to new data. Optimization is a cornerstone of \textit{(DNNs)} training. 
While first-order methods like SGD and Adam are widely used, their limitations in convergence speed, generalization, and sensitivity to hyperparameters justify the exploration of second-order optimizers, which can offer more efficient and robust training for complex neural architectures \cite{Dubey2019,Reyad2023,Habib2020,Gasse2019,Cong2022}.
As shown in Table~\ref{tab:comparison}, first- and second-order optimization methods present different strengths and limitations in \textit{(DNNs)} training \cite{Dubey2019,Reyad2023,Habib2020,Gasse2019,Cong2022}. This paper focuses on evaluating the efficiency of two optimization second-order methods for DNNs, \textit{i)} AdaMuon \cite{kingma2014} and \textit{ii)} K-FAC \cite{martens2015}, with the goal of understanding how their performance can be extrapolated to image classification tasks.

\begin{table}[hbtp]
\caption{Comparison of First- and Second-Order Optimization Methods}
\label{tab:comparison}
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|c|c|p{3.2cm}|}
\hline
\textbf{Method} & \textbf{Evidence} & \textbf{Reasoning} \\
\hline
First-order 
& Strong 
& SGD/Adam struggle with complex loss surfaces and require careful tuning. \\
\hline
Second-order
& Moderate 
& Curvature information enables faster and more robust convergence. \\
\hline
\end{tabular}
\end{table}

%The remainder of this paper is structured as follows. The Introduction???? establishes the theoretical background, formulates the research problem, and outlines the study's motivation. 
% -->LR he cambiado un c=poco este párrafo también, hay que revisar al final que quede tal cual las secciones del paper
The remainder of this paper is structured as follows. Section~\ref{sec:related_work} establishes the theoretical background, formulates the research problem, and outlines the study's motivation. Section~\ref{sec:method} describes the analytical framework, including the theoretical foundations, mathematical derivations, and comparative procedures employed. 
%GS: revisar bien la sección de abajo que número de sección corresponde
Section~\ref{sec:results} presents the empirical findings and quantitative evaluations. These findings are then examined in Section~\ref{sec:discussion}, where their implications and limitations are critically assessed and interpreted, while Section~\ref{sec:conclusion} concludes the work by summarizing the main contributions and then provides potential directions for future research.

\section{Related Work}
\label{sec:related_work}
This section outlines the foundational ideas and techniques necessary to understand the research. It begins with core mathematical tools such as Cross Entropy, Softmax Function, and Gradient Derivation, followed by advanced optimization methods including the Fisher Matrix, K-FAC Method, and the Kronecker Product. It then details the Adam Algorithm, breaking it down into key components like moment estimates, bias corrections, parameter update rules, and conceptual interpretation. Finally, it introduces AdaMuon, a second-order optimization method, along with its main elements such as adaptive moments, sign-stabilization, block-wise orthogonalization, and adaptive step strategies, highlighting computational cost considerations. Together, these concepts provide the theoretical and algorithmic background that supports the study of optimization in the field.

\subsection{Mathematical Backrground}

\subsubsection{Cross-Entropy}
Let q(c) be a true probability distribution defined over a set of classes {1, 2, …, C}, and let p(c) be the distribution predicted by a parameterized model. The cross-entropy is defined as~\cite{martens2015}.
\begin{equation}
\label{eq:H} 
H(q, p) = - \sum_{c=1}^{C} q(c) \, \log p(c)
\end{equation}

%%%% No se olviden de referenciar las ecuaciones -->LR
The expression in ~\eqref{eq:H} quantifies the expected amount of information required to identify a class under the true distribution $q$ when using the optimal code derived from the predicted distribution $p$. In practice, when labeled data are available, the distribution q takes the form of a one-hot vector \cite{martens2015}.
    
This shows that the training cost depends entirely on the probability assigned to the true class. If the model assigns a high probability to the correct class, the loss is low; otherwise, the penalty is high. This property explains the practical effectiveness of cross-entropy as a learning criterion \cite{martens2015}.

\subsubsection{Softmax Function}
The distribution $p(c)$ is not defined arbitrarily, but is obtained from a transformation of the so-called logits $z_c$, which constitutes the linear outputs of the neural network's final layer. The softmax function ensures that the resulting distribution is valid, meaning that it has positive, normalized values \cite{martens2015}.

\begin{equation}
\label{eq:p}
p(c) = \frac{e^{z_c}}{\sum_{k=1}^{C} e^{z_k}}
\end{equation}

The formulation in ~\eqref{eq:p} ensures that the values $p(c)$ form a probability distribution over the set of classes, transforming an arbitrary vector of real numbers into a vector of non-negative values whose sum equals one.

\subsubsection{Gradient Derivation}
Training a model through optimization requires computing the gradient of the loss function with respect to the parameters. The first step is to derive the loss with respect to the logits. Starting from the definition~\cite{martens2015,kaul2022} of

\begin{equation}
    \mathcal{L} = - \sum_{c=1}^{C} q(c) \log p(c)
\end{equation}

by applying the chain rule to the softmax function, the fundamental result can be obtained:
    
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial z_k} = p(k) - q(k).
\end{equation}
    
This result has a straightforward interpretation: if $k$ corresponds to the true class, then $q(k)=1$ and the gradient is $p(k)-1$, which drives the model to increase the probability of that class. Alternatively, for the non-true classes, the gradient reduces to $p(k)$, which tends to decrease the probability assigned to those categories. This simple structure of the gradient greatly facilitates both theoretical analysis and practical implementation in optimization algorithms~\cite{martens2015,kaul2022} .

The Kronecker product between matrices $A \in \mathbb{R}^{m \times n}$ 
and $B \in \mathbb{R}^{p \times q}$ is defined such that

\begin{equation}
\label{eq: Kronecker_Prod}
(A \otimes B)_{(i,k),(j,l)} = A_{ij} B_{kl}
\end{equation}

A key property used in K-FAC is

\begin{equation}
(A \otimes G)^{-1} = A^{-1} \otimes G^{-1}.
\end{equation}

\subsubsection{Derivation for a Linear/Unfolded Convolutional Layer}
Consider a layer with weight parameters $W \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$ \cite{martens2015,kaul2022}. 
For a given example $(x, y)$, the per-example gradient satisfies %%% es esta oración correcta??? -->LR

\begin{equation}
\nabla_W \ell = \delta \, a^\top,
\end{equation}

where $a \in \mathbb{R}^{d_{\text{in}}}$ are the input activations and $\delta \in \mathbb{R^{d_{\text{out}}}}$ are the backpropagated errors with respect to the pre-activations~\cite{martens2015}.

Vectorizing, we obtain
\begin{equation}
\text{vec}(\nabla_W \ell) = a \otimes \delta.
\end{equation}

Hence, the Fisher block associated with $W$ is
\begin{equation}
F_W = \mathbb{E}\!\left[ \, \text{vec}(\nabla_W \ell)\,
   \text{vec}(\nabla_W \ell)^\top \right]
   = \mathbb{E}\!\left[ (aa^\top) \otimes (\delta \delta^\top) \right].
\end{equation}

The K-FAC approximation takes
\begin{equation}
F_W \;\approx\; A \otimes G, 
\quad A := \mathbb{E}[aa^\top], 
\quad G := \mathbb{E}[\delta \delta^\top].
\end{equation}

In the convolutional case, patch re-indexing is applied and the expectations are averaged over spatial locations and the batch. This preserves the same Kronecker structure $A \otimes G$ with $d_{\text{in}} = C_{\text{in}} k_h k_w$ \cite{Reyad2023} \cite{martens2015}.

\subsubsection{Inversion and Update by layer}
Using property $(A \otimes G)^{-1} = A^{-1} \otimes G^{-1}$, the approximate natural gradient update for $W$ is 
\begin{equation}
\begin{aligned}
\text{vec}(\Delta W)
&= -\eta \, (A \otimes G)^{-1} \, \text{vec}(\nabla_W L) \\
&= -\eta \, (A^{-1} \otimes G^{-1}) \, \text{vec}(\nabla_W L).
\end{aligned}
\end{equation}

Its equivalent in matrix form:
\begin{equation}
\Delta W \;\approx\; -\eta \, G^{-1} \, \nabla_W L \, A^{-1}.
\end{equation}

\subsubsection{Damping and Practice}
For stability, factorized damping variants are used \cite{martens2015}:
\begin{equation}
\tilde{A} = A + \pi \sqrt{\lambda}\, I, 
\qquad \tilde{G} = G + \tfrac{1}{\pi}\sqrt{\lambda}\, I,
\end{equation}

with $\pi$ chosen to match scales (e.g., 
$\pi = \sqrt{ \tfrac{\mathrm{tr}(A)/d_{\text{in}}}{\mathrm{tr}(G)/d_{\text{out}}} }$). Inversion of $\tilde{A}, \tilde{G}$ can be performed via Cholesky decomposition.

\subsubsection{Block-wise Orthogonalization} 
Define a partition of the parameters into blocks $\{B_j\}$ (e.g., by layer). 
Let $u_{t,j}$ be the vector formed by the coordinates of $\tilde{m}_t$ in block $B_j$. 
AdaMuon applies a lightweight orthogonalization on $u_{t,j}$, producing $u^{\perp}_{t,j}$, which conceptually is a projection that removes redundant components within the block and reduces collinearity among update dimensions \cite{si2025}.
%% revisar: which conceptually is a projection... es porque cambié una pequeña cosa ahí y hay que ver que no haya perdido el sentido


A practical (and efficient) routine is \cite{si2025}:

    \begin{enumerate}
        \item Normalize 
        $u_{t,j} \leftarrow \frac{u_{t,j}}{\|u_{t,j}\|} 
        \quad \text{if } \|u_{t,j}\| \neq 0.$
        \item Project out historical subspaces $S_{t-1,j}$ 
        (e.g., historical means or principal directions):
        $u^{\perp}_{t,j} = u_{t,j} - P_{S_{t-1,j}}(u_{t,j}).$
    \end{enumerate}

Let $\text{ortho}(\cdot)$ denote the operation which produces $u^{\perp}_{t,j}$. The efficient version described in the preprint has complexity $\mathcal{O}(|B_j|^2)$ per block (not quadratic in the dimension of the global parameter $P$).



\subsection{First-Order Optimization Algorithms}  

\subsubsection{Adam Algorithm}
Adam is a widely used stochastic optimization algorithm for neural networks. It combines momentum with adaptive learning rates, using estimates of the first- and second-order moments of the gradient to update the model parameters efficiently. Let $\theta \in \mathbb{R}^{d} $be the model parameters and$ L(\theta)$ the loss function to minimize. At each iteration $t$, a stochastic gradient estimate is computed on a minibatch of data \cite{Reyad2023}:

    \begin{equation}
        g_t = \nabla_\theta L(\theta_t)
    \end{equation}

\paragraph{Moment Estimates}
Adam maintains two moving averages of gradients \cite{Reyad2023}:

    \begin{equation}
        m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t, 
\quad
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \label{eq: ADAM_EQ_GRAD}
    \end{equation}
    
\paragraph{Bias Corrections.}
Since $m_t$ and $v_t$ are initialized at zero, bias corrections are applied \cite{Reyad2023}:
    \begin{equation}
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, 
\quad
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
    \end{equation}
    
\paragraph{Parameter update rule}
Parameters are updated via:
 \begin{equation}
 \label{eq: Parameter}
     \theta_{t+1} = \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
 \end{equation}
 where $\alpha > 0$ is the learning rate and $\epsilon>0$ ensures numerical stability.
 
\paragraph{Conceptual Interpretation}
 In \eqref{eq: Parameter} $\hat{m}_t$ acts as a smoothed gradient, reducing stochastic variance like momentum. $\hat{v}_t$ adapts the step size inversely to gradient magnitude: coordinates with large gradients take smaller steps, while those with small gradients take larger steps \cite{Reyad2023}.

\paragraph{Computational Cost}
Adam's computational cost scales linearly based on the total number of parameters used for its definition. 

\subsection{Second-Order Optimization Algorithms}

\subsubsection{Fisher Matrix and K-FAC Algorithm}
Optimizing deep neural networks cannot rely solely on the direct gradient. Second-order methods, which incorporate curvature information of the loss function, such as those based on the Fisher matrix, provide substantial improvements. The Fisher matrix, defined as the expected outer product of the gradient of the log-likelihood, encodes information about the geometry of the parameter space \cite{martens2015}:

\begin{equation}
F = \mathbb{E} \Big[ \nabla_\theta \log p(y \mid x; \theta) \, \nabla_\theta \log p(y \mid x; \theta)^\top \Big]
\end{equation}

However, the exact computation and storage of $F$ becomes infeasible for large-scale models due to its size. To address this obstacle, an approximation based on Kronecker-factorization is employed, giving rise to the method known as Kronecker-Factored Approximate Curvature (K-FAC) \cite{martens2015}.

The K-FAC method approximates the Fisher matrix of neural network layers as a Kronecker product of two smaller matrices \eqref{eq: Kronecker_Prod} (input activations and output gradients). This structured decomposition greatly lowers computational and storage costs while preserving a good curvature approximation \cite{martens2015}.

\paragraph{Algorithmic Complexity (Symbolic)} For a layer with dimensions $(d_{\text{in}}, d_{\text{out}})$ and an effective minibatch size $N$ (for convolution, $N = m|U|$), the cost of constructing the estimators is \cite{martens2015} \begin{equation} \text{cost}(A) \sim \mathcal{O}(N d_{\text{in}}^2), \qquad \text{cost}(G) \sim \mathcal{O}(N d_{\text{out}}^2). \end{equation} The inversion (factorization) of $\tilde{A}, \tilde{G}$ costs \begin{equation} \text{cost}_{\text{inv}} \sim \mathcal{O}(d_{\text{in}}^3 + d_{\text{out}}^3), \end{equation} and the application of the preconditioner (product $G^{-1} \nabla_W L A^{-1}$) costs approximately $\mathcal{O}(d_{\text{in}} d_{\text{out}} (d_{\text{in}} + d_{\text{out}})).$ In practice, the inversion is amortized every $T_{\text{inv}}$ iterations.

    
\subsubsection{AdaMuon Algorithm}
Let $g_t$ denote the gradient (per parameter) at step $t$. All operations are per element (vectorized over parameters), except where block operations are explicitly indicated.

\subsubsection{Adaptive Moments.} AdaMuon maintains two elementary moments (as in Adam) \cite{si2025,Reyad2023}. However, AdaMuon's approach differs to what Adam specifes in Equation~\eqref{eq: ADAM_EQ_GRAD}. In this scenario, these operations are carried within the context of information geometry; thus, the steps taken are adapted based on the informational space.

\paragraph{Sign-stabilization of the Momentum} 
AdaMuon applies a sign transformation to the momentum (element by element) \cite{si2025}, 
which can be written generically as

\begin{equation}
\tilde{m}_t = \text{sign}(m_t) \odot f(|m_t|),
\end{equation}

where $f$ is a monotonic function (for example $f(s) = s^\alpha$ with $\alpha \in (0,1]$) that attenuates very small components. In the basic variant, $f(s) = s$, and only of $f$$\text{sign}(m_t)$ is used in practical combinations. This step reduces sensitivity to small oscillations while preserving the dominant  direction per element.
    
\paragraph{Adaptive Step and RMS-Aligned Rescaling}
After orthogonalization, AdaMuon forms the adapted direction per component \cite{si2025}:
\begin{equation}
\hat{d}_t = \frac{u_t^\perp}{\sqrt{v_t + \varepsilon}},
\end{equation}

where $u_t^\perp$ is the vector formed by concatenated blocks $u_{t,j}^\perp$, and $\varepsilon > 0$ prevents division by zero.

Finally, a rescaling is applied so that the RMS of $\hat{d}_t$ matches a reference (e.g., the RMS that Adam would give without orthogonalization). Let

\begin{equation}
\text{rms}(x) := \sqrt{\frac{1}{P} \sum_{i=1}^{P} x_i^2}.
\end{equation}

The following expression can be defined
\begin{equation}
\Delta_t = -\eta \frac{\text{rms}_{\text{ref}}}{\text{rms}(\hat{d}_t) + \delta} \, \hat{d}_t,
\end{equation}

where $\text{rms}_{\text{ref}}$ is chosen (online) so that the step magnitude is comparable to Adam under similar conditions. The effect is that AdaMuon modifies the direction (orthogonal) but maintains a practical step scale \cite{si2025}.

\paragraph{Computational Cost per Step (Summary)}

\begin{itemize}
    \item Computation of $m_t, v_t$: $O(P)$ (element-wise, very cheap).  
    \item Sign-stabilization: $O(P)$.  
    \item Block-wise orthogonalization: $\sum_j O(|B_j|^2)$; if the blocks are layers with sizes $b_j$, cost $\sum_j b_j^2$.  
    \item RMS scaling and update: $O(P)$.
\end{itemize}

Comparing this structure with K-FAC, AdaMuon avoids cubic inversions per block at the cost of a quadratic orthogonalization per block.



\section{Method}
\label{sec:method}

This section focuses on analyzing and comparing the computational efficiency of optimization techniques. It includes a Comparative Cost Analysis, where both symbolic synthesis and representative numerical examples are used to examine the formal and practical implications of the methods, followed by a practical interpretation of their performance. Additionally, it explores Efficiency per Step in terms of loss progress, introducing a modeling approach for the parameter, which helps quantify and evaluate how effectively each optimization method advances toward convergence. Together, these components provide a rigorous framework for assessing both the theoretical cost and practical efficiency of the algorithms under study.

\subsection{Theoretical Comparative Cost Analysis}

\subsubsection*{Symbolic Synthesis}

For a layered network $l = 1, \dots, L$, defined by layer dimensions $(d^{(l)}_{\text{in}}, d^{(l)}_{\text{out}})$ and blocks in AdaMuon with sizes $b^{(l)} = d^{(l)}_{\text{in}} d^{(l)}_{\text{out}}$ (or a finer partition), the following measures of computational cost per iteration (asymptotic FLOPs) are considered:

\textbf{K-FAC:} Building $A_l, G_l$ costs $O\big(N (d^{(l)}_{\text{in}})^2 + N (d^{(l)}_{\text{out}})^2 \big)$; inversion costs $O\big( (d^{(l)}_{\text{in}})^3 + (d^{(l)}_{\text{out}})^3 \big)$ (every $T_{\text{inv}}$ iterations). The amortized cost per iteration of the inversion is  

\begin{equation}
\frac{1}{T_{\text{inv}}} O(d_{\text{in}}^3 + d_{\text{out}}^3).
\end{equation}

\textbf{AdaMuon:} The computational cost of block-wise orthogonalization depends on the scope of application. Full block space orthogonalization costs $O\!\left((b^{(l)})^2\right)$, while restricting it to output dimension vectors reduces the cost to $O(d_{\text{out}}^2 \cdot d_{\text{in}})$. In practice, the method applies orthogonalization at the layer level over the parameter vector.
 
\begin{equation}
\sum_l (b^{(l)})^2.
\end{equation}

\subsubsection{Representative Numerical Example}

Consider a simple and representative example: a dense layer with $d_{\text{in}} = 1024$, $d_{\text{out}} = 1024$. By calculating the basic quantities one can obtain:
\begin{equation}
    d_{\text{in}}^2 = 1024^2 = 1{,}048{,}576, \quad d_{\text{in}}^3 = 1024^3 = 1{,}073{,}741{,}824.
\end{equation}
    
    Therefore,
\begin{equation}
    d_{\text{in}}^3 + d_{\text{out}}^3 = 2 \times 1{,}073{,}741{,}824 = 2{,}147{,}483{,}648,
\end{equation}

    while
\begin{equation}
    d_{\text{in}}^2 + d_{\text{out}}^2 = 2 \times 1{,}048{,}576 = 2{,}097{,}152.
\end{equation}

    
The ratio between the amortized cubic cost and the quadratic cost is
\begin{equation}
    \frac{d_{\text{in}}^3 + d_{\text{out}}^3}{d_{\text{in}}^2 + d_{\text{out}}^2} 
    \approx \frac{2.147 \times 10^9}{2.098 \times 10^6} \approx 1024.
\end{equation}
    
That is, for this layer, direct (cubic) inversion costs roughly $10^3$ times more FLOPs than forming the Gram matrices (the quadratic terms). This illustrates the enormous difference between cubic and quadratic costs for layers of dimension $\sim 1000$.
    
\subsubsection{Practical Interpretation}
\begin{itemize}
    \item K-FAC occasionally pays the cost of $O(d^3)$ per layer to invert $\tilde{A}, \tilde{G}$. Even when amortized, this remains expensive for very large layers (such as large fully-connected layers).
    \item AdaMuon, by avoiding cubic inversions, replaces this cost with block-wise orthogonalization (typically costing $O(b^2)$ per block). If the blocks are much smaller than $d^3$ (e.g., $b \sim d$ and applied per filter), AdaMuon will be much cheaper per iteration.
\end{itemize}

\subsection{Empirical Comparative Cost Analysis}
This study employed a controlled computational experiment to compare first-order and second-order optimization algorithms in deep learning training. The experimental design and workload selection were strictly modeled after established benchmarking methodologies, specifically the MLCommons Algorithmic Efficiency (AlgoPerf) benchmark \cite{mlcommons2023} and the Deep Learning Optimizer Benchmark Suite (DeepOBS) \cite{schneider2018deepobs}. Following these formal paradigms, the independent variable was the optimization method, including Adam, AdaMuon, and K-FAC, while network architecture, data preprocessing, hardware configuration, and training protocol were uniformly standardized across comparisons. To evaluate the generalizability and robustness of the optimizers under realistic constraints, experiments were conducted on numerical, image, and categorical datasets using multiple neural network architectures ranging from multi-layer perceptrons \textit{(MLPs)} to deep residual networks \textit{(ResNets)}. The main outcome measures were per-iteration computational cost, peak memory usage, convergence dynamics, and final classification accuracy.

\begin{table}[!b]
\caption{Dataset–Model Configurations Used in the Benchmark}
\label{tab:workloads}
\centering
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{|l|l|c|r|}
\hline
\textbf{Dataset} & \textbf{Model} & \textbf{Epochs} & \textbf{Parameters} \\ \hline
UCI Adult        & MLP (256--128--64) & 40 & 70,082 \\ \hline
Covertype        & MLP (512--256--128) & 100 & 195,079 \\ \hline
CIFAR-10         & ResNet-18 & 100 & 11,173,962 \\ \hline
CIFAR-100        & ResNet-34 & 80 & 21,328,292 \\ \hline
Fashion-MNIST    & SimpleCNN & 50 & 857,738 \\ \hline
SVHN             & Wide ResNet-16-4 & 50 & 2,748,890 \\ \hline
\end{tabular}
\end{table}

The optimization algorithms used were Adam, AdaMuon, and K-FAC. Their hyperparameters were defined within a configuration file to be held constant across seeds for each workload. All methods were trained using a cosine learning rate schedule.

All three optimizers were evaluated across the six datasets with 5 random seeds, where each dataset–model pair was trained with every optimizer using each seed and every run was executed separately and treated as an independent experiment. This ensures that every optimizer is evaluated under the same conditions and that results are not dependent on a single random initialization.

Each training run was executed as a separate process on a single assigned GPU, with its own log file and results directory. Progress was tracked during execution, including completed, failed, and skipped runs, as well as total runtime.

\paragraph{Metrics}
During training, per-epoch metrics were recorded, including:
\begin{itemize}
\item Training loss
\item Validation loss
\item Validation accuracy
\end{itemize}
Upon completion, summary statistics were computed:
\begin{itemize}
\item Best validation accuracy and corresponding epoch
\item Final validation accuracy
\item Total training time (s)
\item  Average epoch time (s)
\item Peak GPU memory usage (GB)
\item Average throughput (samples/sec)
\end{itemize}

\paragraph{Device Configuration}
The relevant hardware and software specifications used for the experimentation are the following:
\begin{itemize}
\item  GPU model: NVIDIA A16
\item  Number of GPUs: 8
\item  Memory per GPU: 16 GB
\item  Driver version: 535.274.02
\item  CUDA version: 12.2
\end{itemize}

\subsection{Implementation Details}
All experiments were implemented in Python using the PyTorch deep learning framework. Training was executed on NVIDIA GPUs with CUDA acceleration. Each experiment was launched as an independent process with explicit GPU assignment, enabling controlled multi-GPU parallel execution.

\paragraph{Configuration}
All workload specifications were defined in YAML configuration files containing: Dataset parameters, model architecture parameters, training schedule, and optimizer-specific hyperparameters. The optimizer type was selected at runtime.

\paragraph{Reproducibility}
Each optimizer along with its respective dataset was evaluated using five different fixed seeds. For each training run, the seed controlled data shuffling and model weight initialization, so that the training process was deterministic.

\paragraph{Data Loading}
Data loaders were constructed using workload-specific batch size, number of worker processes, and dataset root directory. The number of training samples was recorded and used to compute the metrics.

\paragraph{Model Instantiation}
Models were created using a common interface that built the selected architecture. The input and output sizes were automatically adjusted to match the dataset. Each model was moved to the assigned GPU, and the total number of trainable parameters was recorded for every run.

\paragraph{Optimizer Implementation}
The optimizers were implemented as follows. Py-Torch built-in algorithm was used for Adam, whereas AdaMuon, and K-FAC used custom python implementations based on their algorithmic formulations. Their hyperparameters were read directly from the configuration file and remained fixed across seeds.

\paragraph{Training Procedure}
Training was conducted using a dedicated Trainer module that handled forward propagation, loss computation, backpropagation, optimizer update step, and learning rate scheduling.
The training used the number of epochs specified in Table~\ref{tab:workloads}, recording the following performance metrics throughout the process:
\begin{itemize}

\item Total training time (wall-clock)
\item Average epoch time
\item Peak GPU memory usage
\item Average throughput (samples per second)
\end{itemize}

\section{Results}

This section presents a comprehensive empirical evaluation of first-order and second-order optimization methods across multiple datasets and \textit{(DNNs)}. The analysis is structured along three areas: computational efficiency, memory behavior, and predictive performance. Experiments cover categorical classification, image, and numerical workloads, including shallow multilayer perceptrons \textit{(MLPs)} and convolutional neural networks \textit{(CNNs)} of varying depth and capacity.

\begin{figure*}[t]
    \centering
    % Replace 'my_chart' with the actual name of your PDF file
    \includegraphics[width=\linewidth]{bars_adult_mlp}
    \caption{This is the caption for my PDF image.}
    \label{fig:bars_adult_mlp}
\end{figure*}

\begin{figure*}[t]
    \centering
    % Replace 'my_chart' with the actual name of your PDF file
    \includegraphics[width=\linewidth]{convergence_adult_mlp}
    \caption{This is the caption for my PDF image.}
    \label{fig:convergence_adult_mlp}
\end{figure*}

\begin{figure*}[b]
    \centering
    % Replace 'my_chart' with the actual name of your PDF file
    \includegraphics[width=\linewidth]{epoch_time_comparison}
    \caption{This is the caption for my PDF image.}
    \label{fig:epoch_time_comparison}
\end{figure*}

\begin{table*}[b]
\caption{Summary of Benchmark Results (mean $\pm$ std over 5 seeds)}
\centering\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Workload} & \textbf{Optimizer} & \textbf{Best Val Acc} & \textbf{Time (s)} & \textbf{Memory (GB)} & \textbf{Throughput} \\
\hline
adult\_mlp & Adam & $0.8568 \pm 0.0021$ & $49.8 \pm 17.0$ & $0.02 \pm 0.00$ & $39245 \pm 14142$ \\
adult\_mlp & AdaMuon & $0.8577 \pm 0.0029$ & $51.1 \pm 1.1$ & $0.02 \pm 0.00$ & $33728 \pm 772$ \\
adult\_mlp & K-FAC & $0.8520 \pm 0.0038$ & $66.0 \pm 21.5$ & $0.02 \pm 0.00$ & $29022 \pm 9278$ \\
\hline
cifar100\_resnet34 & Adam & $0.7220 \pm 0.0045$ & $12335.8 \pm 4449.3$ & $1.33 \pm 0.00$ & $411 \pm 129$ \\
cifar100\_resnet34 & AdaMuon & $0.7446 \pm 0.0041$ & $18373.3 \pm 4145.1$ & $1.33 \pm 0.00$ & $265 \pm 90$ \\
cifar100\_resnet34 & K-FAC & $0.2248 \pm 0.0676$ & $22720.8 \pm 2105.1$ & $7.39 \pm 0.00$ & $202 \pm 12$ \\
\hline
cifar10\_resnet18 & Adam & $0.9361 \pm 0.0027$ & $6698.0 \pm 2477.6$ & $0.79 \pm 0.00$ & $763 \pm 245$ \\
cifar10\_resnet18 & AdaMuon & $0.9487 \pm 0.0017$ & $8831.6 \pm 2795.9$ & $0.79 \pm 0.00$ & $576 \pm 209$ \\
cifar10\_resnet18 & K-FAC & $0.5613 \pm 0.0597$ & $20264.1 \pm 4597.9$ & $4.26 \pm 0.00$ & $241 \pm 83$ \\
\hline
covertype\_mlp & Adam & $0.9157 \pm 0.0013$ & $343.4 \pm 119.0$ & $0.03 \pm 0.00$ & $65642 \pm 18169$ \\
covertype\_mlp & AdaMuon & $0.9186 \pm 0.0018$ & $382.5 \pm 65.7$ & $0.03 \pm 0.00$ & $54612 \pm 7550$ \\
covertype\_mlp & K-FAC & $0.6848 \pm 0.0987$ & $459.5 \pm 121.6$ & $0.04 \pm 0.00$ & $47681 \pm 12741$ \\
\hline
fmnist\_simplecnn & Adam & $0.9355 \pm 0.0023$ & $150.8 \pm 1.8$ & $0.18 \pm 0.00$ & $14401 \pm 167$ \\
fmnist\_simplecnn & AdaMuon & $0.8634 \pm 0.0052$ & $226.5 \pm 64.7$ & $0.18 \pm 0.00$ & $10590 \pm 2351$ \\
fmnist\_simplecnn & K-FAC & $0.0999 \pm 0.0051$ & $147053.8 \pm 8059.4$ & $0.50 \pm 0.00$ & $17 \pm 1$ \\
\hline
svhn\_wrn164 & Adam & $0.9577 \pm 0.0027$ & $5115.7 \pm 822.3$ & $0.56 \pm 0.00$ & $1083 \pm 150$ \\
svhn\_wrn164 & AdaMuon & $0.9596 \pm 0.0025$ & $6601.6 \pm 1831.4$ & $0.56 \pm 0.00$ & $845 \pm 165$ \\
\hline
\end{tabular}
\label{tab:benchmark_results}
\end{table*}

\section{Discussion}
\label{sec:discussion}

This study provided a rigorous theoretical and empirical comparison of two second-order optimization strategies for CNNs: Kronecker-Factored Approximate Curvature (K-FAC) and AdaMuon. Our analysis considered their mathematical underpinnings, computational complexity, and performance on image classification with ResNet-18 on the Oxford–IIIT Pet dataset.

\subsubsection*{Mathematical trade-offs}
From a theoretical standpoint, K-FAC explicitly approximates the natural gradient by factorizing the Fisher information matrix into Kronecker products $A \otimes G$. When this factorization is accurate, the resulting updates closely align with the natural gradient, enabling potentially faster convergence in terms of iteration count. However, its dominant cost arises from cubic matrix inversions per layer, amortized over $T_{\text{inv}}$ iterations, which scales poorly with increasing layer dimensionality. In contrast, AdaMuon replaces cubic inversions with block-wise orthogonalization and RMS-aligned rescaling. Its cost grows quadratically with block size, making it substantially cheaper for large layers (e.g., $d=1024$ where $O(d^3)/O(d^2) \approx 10^3$). Thus, even if K-FAC yields more informative steps per iteration, AdaMuon can achieve greater reduction in loss per unit of wall-clock time by avoiding cubic overheads.

\subsubsection*{Empirical performance}
The carried experiments confirmed these theoretical expectations. K-FAC achieved strong peak validation accuracy ($0.9987 \pm 0.0031$) but incurred the largest computational overhead, with iteration times averaging $180$\,ms and peak memory consumption of $24.5 \pm 4.9$\,GB. AdaMuon matched the best accuracy ($1.0000$ at peak; final $0.9926 \pm 0.0061$) while reducing peak memory to $14.0 \pm 1.2$\,GB and total training time to $1.1 \pm 0.4$ minutes, compared to $1.7 \pm 0.5$ minutes for K-FAC. AdamW remained the strongest first-order baseline, delivering perfect accuracy ($1.0000$) with minimal resource usage, whereas SGD was fastest per iteration ($\sim$82\,ms) but significantly underperformed in accuracy (final $0.4048 \pm 0.0552$). Statistical tests confirmed that all advanced methods (AdamW, K-FAC, AdaMuon) significantly outperformed SGD, while differences among them were not statistically significant.

\subsubsection*{Interpretation and practical implications}

The results demonstrate that AdaMuon provides a more favorable efficiency–accuracy trade-off under large-layer, single-GPU conditions. Its quadratic per-block complexity explains its empirical advantages in both speed and memory, while K-FAC remains limited by the scalability of factor construction and inversion. Nevertheless, K-FAC retains theoretical appeal in regimes with smaller layer dimensions or when its Kronecker factorization is highly accurate and amortization of inversions is feasible. AdaMuon, on the other hand, offers robustness, adaptivity, and near–first-order cost profiles, positioning it as a practical alternative for real-world CNN training workloads.

\subsubsection*{Limitations and future work}
This study is limited to ResNet-18 on a medium-scale dataset and single-GPU training. Further research should evaluate these methods on larger architectures (e.g., transformers or deeper CNNs), multi-GPU and distributed settings, and across diverse datasets. Additionally, sensitivity analyses of hyperparameters such as damping (K-FAC) and block size (AdaMuon) would provide deeper insights into stability and generalization behavior.

\section{Conclusion}
\label{sec:conclusion}
Overall, the combined mathematical derivations and empirical findings converge to a consistent conclusion: \textbf{AdaMuon} achieves superior efficiency in practice by avoiding cubic matrix inversions, while preserving second-order benefits through adaptive moments and block-wise orthogonalization. K-FAC remains valuable for its principled approximation of the natural gradient but is constrained by scalability. AdaMuon therefore emerges as a practical and scalable second-order optimizer that balances theoretical grounding with efficiency, making it a compelling alternative for modern deep learning tasks.



\begin{thebibliography}{10}

\bibitem{maslej2025}
Nestor Maslej et al.,  
\textit{Artificial Intelligence Index Report 2025},  
arXiv (Cornell University), April 2025.  
DOI: 10.48550/arxiv.2504.07139.

\bibitem{patel2025}
Suyash Patel,  
\textit{Global AI Tool Adoption Across Industries: A Comprehensive Analysis of Trends and Patterns (2023--2025)},  
August 2025.  
DOI: 10.13140/RG.2.2.20656.06407.

\bibitem{alrajeh2025}
Mariam Alrajeh and Aida Al-Samawi,  
\textit{Deepfake image classification using decision (Binary) tree deep learning},  
Journal of Sensor and Actuator Networks, vol. 14, no. 2, p. 40, 2025.  
DOI: 10.3390/jsan14020040.

\bibitem{elngar2021}
Ahmed A. Elngar, Mohamed Arafa, Amar Fathy, Basma Moustafa, Omar Mahmoud, Mohamed Shaban, and Nehal Fawzy,  
\textit{Image classification based on CNN: a survey},  
Journal of Cybersecurity and Information Management, pp. 18--50, 2021.  
DOI: 10.54216/jcim.060102.

\bibitem{ersavas2024}
T. Ersavas, M. A. Smith, and J. S. Mattick,  
\textit{Novel applications of Convolutional Neural Networks in the age of Transformers},  
Scientific Reports, vol. 14, no. 1, p. 10000, 2024.  
DOI: 10.1038/s41598-024-60709-z.

\bibitem{Dubey2019}
Dubey, S., Chakraborty, S., Roy, S., Mukherjee, S., Singh, S., Chaudhuri, B.: 
diffGrad: An Optimization Method for Convolutional Neural Networks. 
IEEE Transactions on Neural Networks and Learning Systems \textbf{31}, 4500--4511 (2019). 
DOI: 10.1109/TNNLS.2019.2955777.

\bibitem{Reyad2023}
Reyad, M., Sarhan, A., Arafa, M.: 
A modified Adam algorithm for deep neural network optimization. 
Neural Computing and Applications \textbf{35}, 17095--17112 (2023). 
DOI: 10.1007/s00521-023-08568-z.

\bibitem{Habib2020}
Habib, G., Qureshi, S.: 
Optimization and acceleration of convolutional neural networks: A survey. 
Journal of King Saud University - Computer and Information Sciences \textbf{34}, 4244--4268 (2020). 
DOI: 10.1016/j.jksuci.2020.10.004.

\bibitem{Gasse2019}
Gasse, M., Ch{\'e}telat, D., Ferroni, N., Charlin, L., Lodi, A.: 
Exact Combinatorial Optimization with Graph Convolutional Neural Networks. 
In: Advances in Neural Information Processing Systems (NeurIPS), pp. 15554--15566 (2019).

\bibitem{Cong2022}
Cong, S., Zhou, Y.: 
A review of convolutional neural network architectures and their optimizations. 
Artificial Intelligence Review \textbf{56}, 1905--1969 (2022). 
DOI: 10.1007/s10462-022-10213-5.

\bibitem{kingma2014}
Diederik P. Kingma and Jimmy Lei Ba,  
\textit{Adam: A method for stochastic optimization},  
arXiv preprint, 2014.  
DOI: 10.48550/arxiv.1412.6980.

\bibitem{martens2015}
James Martens and Roger Grosse,  
\textit{Optimizing Neural Networks with Kronecker-factored Approximate Curvature},  
Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.  

\bibitem{si2025}
Chongjie Si, Debing Zhang, and Wei Shen,  
\textit{AdaMuon: Adaptive Muon Optimizer},  
arXiv preprint, 2025.  

\bibitem{kaul2022}
Piyush Kaul and Brejesh Lall,  
\textit{Projective Fisher information for natural gradient descent},  
IEEE Transactions on Artificial Intelligence, vol. 4, no. 2, pp. 304--314, 2022.  
DOI: 10.1109/tai.2022.3153593.

\bibitem{schneider2018deepobs}
Frank Schneider, Lukas Balles, and Philipp Hennig,
\textit{DeepOBS: A Deep Learning Optimizer Benchmark Suite},
International Conference on Learning Representations, 2019.
URL: https://openreview.net/forum?id=rJg6ssC5Y7

\bibitem{mlcommons2023}
George E. Dahl et al.,
\textit{Benchmarking Neural Network Training Algorithms},
arXiv preprint arXiv:2306.07179, 2023.
URL: https://arxiv.org/abs/2306.07179
\end{thebibliography}

\vspace{12pt}

\end{document}
